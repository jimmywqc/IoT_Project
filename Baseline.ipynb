{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Files\\\\Jupyter\\\\IoT\\\\赛题二-初赛'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Files\\\\Jupyter\\\\IoT\\\\赛题二-初赛')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_train=pd.read_csv('.\\Train\\Size.csv')\n",
    "df_train_spc=pd.read_csv('.\\Train\\data_spc.csv')\n",
    "df_test_spc=pd.read_csv('.\\Test\\data_spc.csv')\n",
    "\n",
    "df_train_spc.drop(columns='remark', inplace=True)\n",
    "df_test_spc.drop(columns='remark', inplace=True)\n",
    "\n",
    "variables1=['Sensor1', 'Sensor2', 'Sensor3', 'IJ', 'Sensor5',\n",
    "       'Sensor6', 'MouldTemp1', 'MouldTemp2', 'MouldTemp3', 'MouldTemp4',\n",
    "       'MouldTemp5', 'MouldTemp9', 'MouldTemp10', 'MouldTemp11', 'MouldTemp12',\n",
    "       'MouldTemp13', 'MouldTemp14', 'Sensor8', 'MouldFlow1', 'MouldFlow2',\n",
    "       'MouldFlow3', 'SP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ZIP=zipfile.ZipFile('.\\Train\\传感器高频数据.zip')\n",
    "file_list=TRAIN_ZIP.namelist()\n",
    "feature_n = len(variables1)\n",
    "features_ = np.empty([len(file_list), feature_n])\n",
    "times_ = []\n",
    "mold_id_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_columns(variables):\n",
    "    f_cols = []\n",
    "    for v in variables:\n",
    "        f_cols.append(v + '_mean')\n",
    "    return f_cols\n",
    "\n",
    "def stage_features(df, variables):\n",
    "    avg = []\n",
    "    tmp_df = df.loc[:, variables]\n",
    "    for v in variables:\n",
    "        tmp_avg = tmp_df[v].mean()\n",
    "        avg.append(tmp_avg)    \n",
    "    return np.array(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_n = len(variables1)\n",
    "features_ = np.empty([len(file_list), feature_n])\n",
    "times_ = []\n",
    "mold_id_ = []\n",
    "for i,f in enumerate(file_list):\n",
    "    df=pd.read_csv(TRAIN_ZIP.open(f))\n",
    "    tmp = f.split('_')\n",
    "    ti = tmp[2]\n",
    "    mold_id = tmp[3].replace('.csv', '')\n",
    "    times_.append(str(ti))\n",
    "    mold_id_.append(int(mold_id))\n",
    "    if len(df) == 0:\n",
    "        features_[i] = [None for j in range(feature_n)]\n",
    "    else:\n",
    "        features_[i] = stage_features(df, variables1)\n",
    "f_cols = feature_columns(variables1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_HIG = pd.DataFrame(features_, columns=f_cols)\n",
    "TRAIN_HIG['Time'] = times_\n",
    "TRAIN_HIG['Id'] = mold_id_\n",
    "TRAIN_HIG = TRAIN_HIG[['Id', 'Time'] + f_cols]\n",
    "TRAIN_ZIP.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ZIP=zipfile.ZipFile('.\\Test\\传感器高频数据.zip')\n",
    "file_list=TEST_ZIP.namelist()\n",
    "feature_n = len(variables1)\n",
    "features_ = np.empty([len(file_list), feature_n])\n",
    "times_ = []\n",
    "mold_id_ = []\n",
    "for i,f in enumerate(file_list):\n",
    "    df=pd.read_csv(TEST_ZIP.open(f))\n",
    "    tmp = f.split('_')\n",
    "    ti = tmp[2]\n",
    "    mold_id = tmp[3].replace('.csv', '')\n",
    "    times_.append(str(ti))\n",
    "    mold_id_.append(int(mold_id))\n",
    "    if len(df) == 0:\n",
    "        features_[i] = [None for j in range(feature_n)]\n",
    "    else:\n",
    "        features_[i] = stage_features(df, variables1)\n",
    "f_cols = feature_columns(variables1)\n",
    "TEST_HIG = pd.DataFrame(features_, columns=f_cols)\n",
    "TEST_HIG['Time'] = times_\n",
    "TEST_HIG['Id'] = mold_id_\n",
    "TEST_HIG = TEST_HIG[['Id', 'Time'] + f_cols]\n",
    "TEST_ZIP.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_HIG.rename(columns={'Time':'spcTime'}, inplace=True)\n",
    "TEST_HIG.rename(columns={'Time':'spcTime'}, inplace=True)\n",
    "\n",
    "df_train_spc['spcTime']=df_train_spc['spcTime'].apply(int)\n",
    "df_train_spc['spcTime']=df_train_spc['spcTime'].apply(str)\n",
    "df_test_spc['spcTime']=df_test_spc['spcTime'].apply(int)\n",
    "df_test_spc['spcTime']=df_test_spc['spcTime'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TRAIN=TRAIN_HIG.merge(df_train_spc, how='outer', on=['Id', 'spcTime']) \n",
    "df_TEST=TEST_HIG.merge(df_test_spc, how='outer', on=['Id', 'spcTime']) \n",
    "X_col=[i for i in df_TRAIN.columns if not i in ['Id', 'spcTime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "seed = 1234\n",
    "skf = KFold(n_splits=K, shuffle=True, random_state=seed)\n",
    "lgb_params = {\n",
    "                        'boosting_type': 'gbdt',\n",
    "                        'objective': 'regression',\n",
    "                        'num_leaves': 2**5,\n",
    "                        'subsample': 0.9,\n",
    "                        'learning_rate': 0.05,\n",
    "                        'seed': 2017,\n",
    "                        'nthread': -1\n",
    "             }\n",
    "\n",
    "def mode_(size_i):\n",
    "    predictions = np.zeros(len(X_test))\n",
    "    for i, (train_index, val_index) in enumerate(skf.split(X_train,y_train)):\n",
    "        print(\"fold {}\".format(i))\n",
    "        X_tr, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        lgb_train = lgb.Dataset(X_tr,y_tr)\n",
    "        lgb_val = lgb.Dataset(X_val,y_val)\n",
    "        num_round = 2000\n",
    "        clf = lgb.train(lgb_params, lgb_train, num_round, valid_sets = [lgb_train, lgb_val],verbose_eval=50, \n",
    "                        early_stopping_rounds = 50)\n",
    "        print('best iteration = ',clf.best_iteration)\n",
    "        print(\"*\"*100)\n",
    "        predictions += clf.predict(X_test, num_iteration=clf.best_iteration) / skf.n_splits\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size1\n",
      "fold 0\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004944 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12657\n",
      "[LightGBM] [Info] Number of data points in the train set: 13280, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 300.065686\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's l2: 0.000392653\tvalid_1's l2: 0.000450421\n",
      "[100]\ttraining's l2: 0.00032749\tvalid_1's l2: 0.000426317\n",
      "[150]\ttraining's l2: 0.000294051\tvalid_1's l2: 0.000423149\n",
      "[200]\ttraining's l2: 0.00026527\tvalid_1's l2: 0.000422581\n",
      "Early stopping, best iteration is:\n",
      "[187]\ttraining's l2: 0.0002724\tvalid_1's l2: 0.000422187\n",
      "best iteration =  187\n",
      "****************************************************************************************************\n",
      "fold 1\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12659\n",
      "[LightGBM] [Info] Number of data points in the train set: 13280, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 300.065632\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's l2: 0.000403817\tvalid_1's l2: 0.000396212\n",
      "[100]\ttraining's l2: 0.00033626\tvalid_1's l2: 0.000363746\n",
      "[150]\ttraining's l2: 0.000300714\tvalid_1's l2: 0.000360864\n",
      "[200]\ttraining's l2: 0.000271537\tvalid_1's l2: 0.000358456\n",
      "[250]\ttraining's l2: 0.000246256\tvalid_1's l2: 0.000357951\n",
      "[300]\ttraining's l2: 0.000224678\tvalid_1's l2: 0.000357628\n",
      "Early stopping, best iteration is:\n",
      "[265]\ttraining's l2: 0.000239288\tvalid_1's l2: 0.000357438\n",
      "best iteration =  265\n",
      "****************************************************************************************************\n",
      "fold 2\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005769 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12658\n",
      "[LightGBM] [Info] Number of data points in the train set: 13280, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 300.065281\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's l2: 0.000379454\tvalid_1's l2: 0.000513199\n",
      "[100]\ttraining's l2: 0.000315826\tvalid_1's l2: 0.000493571\n",
      "[150]\ttraining's l2: 0.000283397\tvalid_1's l2: 0.000492215\n",
      "[200]\ttraining's l2: 0.000255261\tvalid_1's l2: 0.000491326\n",
      "[250]\ttraining's l2: 0.000231627\tvalid_1's l2: 0.000491091\n",
      "Early stopping, best iteration is:\n",
      "[222]\ttraining's l2: 0.000244343\tvalid_1's l2: 0.000490301\n",
      "best iteration =  222\n",
      "****************************************************************************************************\n",
      "fold 3\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006419 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12657\n",
      "[LightGBM] [Info] Number of data points in the train set: 13280, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 300.065616\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's l2: 0.000396125\tvalid_1's l2: 0.000427804\n",
      "[100]\ttraining's l2: 0.000330294\tvalid_1's l2: 0.000405197\n",
      "[150]\ttraining's l2: 0.000296477\tvalid_1's l2: 0.000403724\n",
      "[200]\ttraining's l2: 0.000267442\tvalid_1's l2: 0.00040492\n",
      "Early stopping, best iteration is:\n",
      "[158]\ttraining's l2: 0.000291846\tvalid_1's l2: 0.000403473\n",
      "best iteration =  158\n",
      "****************************************************************************************************\n",
      "fold 4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12658\n",
      "[LightGBM] [Info] Number of data points in the train set: 13280, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 300.065386\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's l2: 0.000403037\tvalid_1's l2: 0.000410299\n",
      "[100]\ttraining's l2: 0.000335455\tvalid_1's l2: 0.00038254\n",
      "[150]\ttraining's l2: 0.000299848\tvalid_1's l2: 0.000378852\n",
      "[200]\ttraining's l2: 0.000270036\tvalid_1's l2: 0.00037699\n",
      "[250]\ttraining's l2: 0.000245717\tvalid_1's l2: 0.000375876\n",
      "[300]\ttraining's l2: 0.000223456\tvalid_1's l2: 0.000375957\n",
      "Early stopping, best iteration is:\n",
      "[257]\ttraining's l2: 0.000242029\tvalid_1's l2: 0.000375508\n",
      "best iteration =  257\n",
      "****************************************************************************************************\n",
      "size2\n",
      "fold 0\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005166 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12657\n",
      "[LightGBM] [Info] Number of data points in the train set: 13280, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 200.005224\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's l2: 0.000382361\tvalid_1's l2: 0.000455285\n",
      "[100]\ttraining's l2: 0.000312854\tvalid_1's l2: 0.000412777\n",
      "[150]\ttraining's l2: 0.000280255\tvalid_1's l2: 0.000411478\n",
      "[200]\ttraining's l2: 0.000254289\tvalid_1's l2: 0.000410607\n",
      "[250]\ttraining's l2: 0.000230959\tvalid_1's l2: 0.000410885\n",
      "Early stopping, best iteration is:\n",
      "[230]\ttraining's l2: 0.000239846\tvalid_1's l2: 0.000410001\n",
      "best iteration =  230\n",
      "****************************************************************************************************\n",
      "fold 1\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12659\n",
      "[LightGBM] [Info] Number of data points in the train set: 13280, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 200.005556\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's l2: 0.00039059\tvalid_1's l2: 0.000404865\n",
      "[100]\ttraining's l2: 0.000320048\tvalid_1's l2: 0.00037215\n",
      "[150]\ttraining's l2: 0.000286463\tvalid_1's l2: 0.000367181\n",
      "[200]\ttraining's l2: 0.00025959\tvalid_1's l2: 0.000366294\n",
      "[250]\ttraining's l2: 0.000237292\tvalid_1's l2: 0.000364269\n",
      "[300]\ttraining's l2: 0.000217311\tvalid_1's l2: 0.00036472\n",
      "Early stopping, best iteration is:\n",
      "[252]\ttraining's l2: 0.00023654\tvalid_1's l2: 0.000364169\n",
      "best iteration =  252\n",
      "****************************************************************************************************\n",
      "fold 2\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12658\n",
      "[LightGBM] [Info] Number of data points in the train set: 13280, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 200.005398\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's l2: 0.000382507\tvalid_1's l2: 0.000443104\n",
      "[100]\ttraining's l2: 0.000314242\tvalid_1's l2: 0.000409531\n",
      "[150]\ttraining's l2: 0.000281822\tvalid_1's l2: 0.000403863\n",
      "[200]\ttraining's l2: 0.000256131\tvalid_1's l2: 0.000402609\n",
      "[250]\ttraining's l2: 0.000233721\tvalid_1's l2: 0.000402539\n",
      "[300]\ttraining's l2: 0.000213701\tvalid_1's l2: 0.000401862\n",
      "[350]\ttraining's l2: 0.000195871\tvalid_1's l2: 0.000401758\n",
      "Early stopping, best iteration is:\n",
      "[313]\ttraining's l2: 0.000209122\tvalid_1's l2: 0.000401394\n",
      "best iteration =  313\n",
      "****************************************************************************************************\n",
      "fold 3\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12657\n",
      "[LightGBM] [Info] Number of data points in the train set: 13280, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 200.005755\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's l2: 0.000387549\tvalid_1's l2: 0.000415948\n",
      "[100]\ttraining's l2: 0.000317484\tvalid_1's l2: 0.000388477\n",
      "[150]\ttraining's l2: 0.000284292\tvalid_1's l2: 0.000387758\n",
      "[200]\ttraining's l2: 0.000258325\tvalid_1's l2: 0.000387291\n",
      "[250]\ttraining's l2: 0.000236061\tvalid_1's l2: 0.000386781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\ttraining's l2: 0.000217041\tvalid_1's l2: 0.0003864\n",
      "[350]\ttraining's l2: 0.000198527\tvalid_1's l2: 0.000386536\n",
      "Early stopping, best iteration is:\n",
      "[306]\ttraining's l2: 0.000215026\tvalid_1's l2: 0.00038605\n",
      "best iteration =  306\n",
      "****************************************************************************************************\n",
      "fold 4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005049 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12658\n",
      "[LightGBM] [Info] Number of data points in the train set: 13280, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 200.005326\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's l2: 0.000386223\tvalid_1's l2: 0.00041396\n",
      "[100]\ttraining's l2: 0.000315021\tvalid_1's l2: 0.000389904\n",
      "[150]\ttraining's l2: 0.000280283\tvalid_1's l2: 0.000385669\n",
      "[200]\ttraining's l2: 0.000254307\tvalid_1's l2: 0.000385183\n",
      "[250]\ttraining's l2: 0.000232106\tvalid_1's l2: 0.000385398\n",
      "Early stopping, best iteration is:\n",
      "[214]\ttraining's l2: 0.000247795\tvalid_1's l2: 0.000384682\n",
      "best iteration =  214\n",
      "****************************************************************************************************\n",
      "size3\n",
      "fold 0\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12657\n",
      "[LightGBM] [Info] Number of data points in the train set: 13280, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 200.017734\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's l2: 0.000294265\tvalid_1's l2: 0.000334373\n",
      "[100]\ttraining's l2: 0.00024858\tvalid_1's l2: 0.000314549\n",
      "[150]\ttraining's l2: 0.00022398\tvalid_1's l2: 0.000312728\n",
      "[200]\ttraining's l2: 0.000203763\tvalid_1's l2: 0.0003124\n",
      "[250]\ttraining's l2: 0.000185561\tvalid_1's l2: 0.000311262\n",
      "[300]\ttraining's l2: 0.000169639\tvalid_1's l2: 0.000310605\n",
      "[350]\ttraining's l2: 0.000154604\tvalid_1's l2: 0.000310346\n",
      "Early stopping, best iteration is:\n",
      "[310]\ttraining's l2: 0.000166191\tvalid_1's l2: 0.000310113\n",
      "best iteration =  310\n",
      "****************************************************************************************************\n",
      "fold 1\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12659\n",
      "[LightGBM] [Info] Number of data points in the train set: 13280, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 200.017988\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's l2: 0.000298753\tvalid_1's l2: 0.000308335\n",
      "[100]\ttraining's l2: 0.000252691\tvalid_1's l2: 0.000290907\n",
      "[150]\ttraining's l2: 0.000227873\tvalid_1's l2: 0.000288164\n",
      "[200]\ttraining's l2: 0.000206995\tvalid_1's l2: 0.000287269\n",
      "[250]\ttraining's l2: 0.000187804\tvalid_1's l2: 0.000286553\n",
      "Early stopping, best iteration is:\n",
      "[227]\ttraining's l2: 0.000196291\tvalid_1's l2: 0.000286338\n",
      "best iteration =  227\n",
      "****************************************************************************************************\n",
      "fold 2\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12658\n",
      "[LightGBM] [Info] Number of data points in the train set: 13280, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 200.017528\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's l2: 0.000292706\tvalid_1's l2: 0.000338167\n",
      "[100]\ttraining's l2: 0.000247706\tvalid_1's l2: 0.000320487\n",
      "[150]\ttraining's l2: 0.000222814\tvalid_1's l2: 0.000319291\n",
      "[200]\ttraining's l2: 0.000201964\tvalid_1's l2: 0.000319025\n",
      "[250]\ttraining's l2: 0.000184238\tvalid_1's l2: 0.000318359\n",
      "[300]\ttraining's l2: 0.000168227\tvalid_1's l2: 0.000318242\n",
      "[350]\ttraining's l2: 0.000154503\tvalid_1's l2: 0.000317871\n",
      "[400]\ttraining's l2: 0.000141635\tvalid_1's l2: 0.000317765\n",
      "Early stopping, best iteration is:\n",
      "[362]\ttraining's l2: 0.000151616\tvalid_1's l2: 0.000317523\n",
      "best iteration =  362\n",
      "****************************************************************************************************\n",
      "fold 3\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12657\n",
      "[LightGBM] [Info] Number of data points in the train set: 13280, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 200.017902\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's l2: 0.000293669\tvalid_1's l2: 0.00033834\n",
      "[100]\ttraining's l2: 0.000248547\tvalid_1's l2: 0.000323347\n",
      "[150]\ttraining's l2: 0.0002248\tvalid_1's l2: 0.000322228\n",
      "[200]\ttraining's l2: 0.000204346\tvalid_1's l2: 0.00032055\n",
      "Early stopping, best iteration is:\n",
      "[198]\ttraining's l2: 0.000205355\tvalid_1's l2: 0.00032044\n",
      "best iteration =  198\n",
      "****************************************************************************************************\n",
      "fold 4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12658\n",
      "[LightGBM] [Info] Number of data points in the train set: 13280, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score 200.017682\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's l2: 0.000299316\tvalid_1's l2: 0.00030948\n",
      "[100]\ttraining's l2: 0.000252795\tvalid_1's l2: 0.000293457\n",
      "[150]\ttraining's l2: 0.000226454\tvalid_1's l2: 0.000292102\n",
      "[200]\ttraining's l2: 0.00020535\tvalid_1's l2: 0.000291344\n",
      "Early stopping, best iteration is:\n",
      "[192]\ttraining's l2: 0.000208347\tvalid_1's l2: 0.000290984\n",
      "best iteration =  192\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "sub=pd.read_csv('sub_file.csv')\n",
    "for i in ['size1','size2','size3']:\n",
    "    print(i)\n",
    "    X_train=df_TRAIN[X_col]\n",
    "    y_train=size_train[i]\n",
    "    X_test=df_TEST[X_col]\n",
    "    sub[i]=mode_(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>size1</th>\n",
       "      <th>size2</th>\n",
       "      <th>size3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56689</td>\n",
       "      <td>300.060775</td>\n",
       "      <td>199.993348</td>\n",
       "      <td>199.997687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56691</td>\n",
       "      <td>300.060208</td>\n",
       "      <td>199.965673</td>\n",
       "      <td>199.996706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56697</td>\n",
       "      <td>300.064945</td>\n",
       "      <td>199.964685</td>\n",
       "      <td>199.998738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56698</td>\n",
       "      <td>300.063678</td>\n",
       "      <td>199.964065</td>\n",
       "      <td>199.997576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56701</td>\n",
       "      <td>300.062362</td>\n",
       "      <td>199.960091</td>\n",
       "      <td>199.996479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>175911</td>\n",
       "      <td>300.040092</td>\n",
       "      <td>200.132680</td>\n",
       "      <td>200.045408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3949</th>\n",
       "      <td>175961</td>\n",
       "      <td>300.028621</td>\n",
       "      <td>200.127353</td>\n",
       "      <td>200.038473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3950</th>\n",
       "      <td>176111</td>\n",
       "      <td>300.025214</td>\n",
       "      <td>200.130948</td>\n",
       "      <td>200.046959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3951</th>\n",
       "      <td>176131</td>\n",
       "      <td>300.019501</td>\n",
       "      <td>200.126699</td>\n",
       "      <td>200.035191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3952</th>\n",
       "      <td>176241</td>\n",
       "      <td>300.033112</td>\n",
       "      <td>200.120826</td>\n",
       "      <td>200.030839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3953 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id       size1       size2       size3\n",
       "0      56689  300.060775  199.993348  199.997687\n",
       "1      56691  300.060208  199.965673  199.996706\n",
       "2      56697  300.064945  199.964685  199.998738\n",
       "3      56698  300.063678  199.964065  199.997576\n",
       "4      56701  300.062362  199.960091  199.996479\n",
       "...      ...         ...         ...         ...\n",
       "3948  175911  300.040092  200.132680  200.045408\n",
       "3949  175961  300.028621  200.127353  200.038473\n",
       "3950  176111  300.025214  200.130948  200.046959\n",
       "3951  176131  300.019501  200.126699  200.035191\n",
       "3952  176241  300.033112  200.120826  200.030839\n",
       "\n",
       "[3953 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
